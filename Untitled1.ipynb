{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np    \n",
    "import pandas as pd \n",
    "from nltk.stem.porter import PorterStemmer  \n",
    "from nltk.corpus import stopwords\n",
    "import re \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn import feature_extraction\n",
    "from nltk.tokenize import sent_tokenize \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier , LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score , confusion_matrix , accuracy_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "import sys\n",
    "import warnings\n",
    "from sklearn import svm\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('tableheader (3).csv',header = None,error_bad_lines=False,sep = ';')\n",
    "df1.fillna(0,inplace = True)\n",
    "df1.columns = ['documentID','DocName','max_pages','pageN','maxTables','table_id','tabN','tRowsCount','HCellsCount','HCellTxt','DTagsStr','TtagsStr','clmncellTxt']\n",
    "df1 = df1.loc[df1['TtagsStr'].isin(['Horizontal*','NotPertinent*','Vertical*Interlaced*','Vertical*','Horizontal*Interlaced*','Interlaced*Vertical*','Interlaced*Horizontal*'])]\n",
    "df1.drop(['DTagsStr','table_id'],axis = 1,inplace = True)\n",
    "df1.loc[df1['TtagsStr'].isin(['Vertical*Interlaced*','Interlaced*Vertical*','Vertical*']),'TtagsStr'] = 'Vertical'\n",
    "df1.loc[df1['TtagsStr'].isin(['Horizontal*','Horizontal*Interlaced*','Interlaced*Horizontal*']),'TtagsStr'] = 'Horizontal'\n",
    "df1.loc[df1['TtagsStr']=='NotPertinent*','TtagsStr'] = 'NotPertinent'\n",
    "df = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1['TtagsStr']  == 'Horizontal' , 'clmncellTxt'] = 0\n",
    "df1.loc[df1['TtagsStr']  == 'Horizontal' , 'TtagsStr'] = 'Pertinent'\n",
    "\n",
    "df1.loc[df1['TtagsStr']  == 'Vertical' , 'clmncellTxt'] = 0\n",
    "df1.loc[df1['TtagsStr']  == 'Vertical' , 'TtagsStr'] = 'Pertinent'\n",
    "\n",
    "df1.loc[df1['TtagsStr']  == 'NotPertinent' , 'clmncellTxt'] = 0\n",
    "df1.loc[df1['TtagsStr']  == 'NotPertinent' , 'TtagsStr'] = 'NotPertinent'\n",
    "df1['Header'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['TtagsStr']  == 'Horizontal' , 'HCellTxt'] = 1\n",
    "df.loc[df['TtagsStr']  == 'Horizontal' , 'TtagsStr'] = 'NotPertinent'\n",
    "\n",
    "df.loc[df['TtagsStr']  == 'Vertical' , 'HCellTxt'] = 1\n",
    "df.loc[df['TtagsStr']  == 'Vertical' , 'TtagsStr'] = 'NotPertinent'\n",
    "\n",
    "df.loc[df['TtagsStr']  == 'NotPertinent' , 'HCellTxt'] = 1\n",
    "df.loc[df['TtagsStr']  == 'NotPertinent' , 'TtagsStr'] = 'NotPertinent'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.concat([df1,df],axis = 0)\n",
    "test = test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['TtagsStr'] = np.where(test['TtagsStr'].isin(['NotPertinent']),1,0)\n",
    "headlines = []\n",
    "for row in range(len(test.index)):\n",
    "    headlines.append(' '.join(str(x) for x in test.iloc[row,[8,10]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "corpus = []\n",
    "for i in range(len(headlines)):\n",
    "    rev = re.sub('[^a-zA-Z]',' ',headlines[i])\n",
    "    rev = rev.lower()\n",
    "    rev = rev.split()\n",
    "    rev = [ps.stem(word) for word in rev if word not in set(stopwords.words('italian'))]\n",
    "    rev = ' '.join(rev)\n",
    "    corpus.append(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(corpus)\n",
    "X = train_vectors.toarray()\n",
    "y = test['TtagsStr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X,np.array(test.iloc[:,[2,3,4,5,6,7]])],axis = 1) #Adding tables information into learningset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9937310020456317"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "model = SGDClassifier(alpha =  0.0001,loss = 'log', max_iter =  1000, penalty = 'l2')\n",
    "model.fit(X_train,y_train)\n",
    "calibrator = CalibratedClassifierCV(model, cv='prefit')\n",
    "model = calibrator.fit(X_train, y_train)\n",
    "y_pred = model.predict_proba(X_test)\n",
    "roc_auc_score(y_test,np.log(y_pred[:,1])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
      "[Parallel(n_jobs=5)]: Done   3 tasks      | elapsed:    1.9s\n",
      "[Parallel(n_jobs=5)]: Done   8 tasks      | elapsed:    3.5s\n",
      "[Parallel(n_jobs=5)]: Done  15 tasks      | elapsed:    5.3s\n",
      "[Parallel(n_jobs=5)]: Done  22 tasks      | elapsed:    7.4s\n",
      "[Parallel(n_jobs=5)]: Done  31 tasks      | elapsed:    9.9s\n",
      "[Parallel(n_jobs=5)]: Done  40 tasks      | elapsed:   11.2s\n",
      "[Parallel(n_jobs=5)]: Done  51 tasks      | elapsed:   12.6s\n",
      "[Parallel(n_jobs=5)]: Done  62 tasks      | elapsed:   14.0s\n",
      "[Parallel(n_jobs=5)]: Done  75 tasks      | elapsed:   15.3s\n",
      "[Parallel(n_jobs=5)]: Done  88 tasks      | elapsed:   16.5s\n",
      "[Parallel(n_jobs=5)]: Done 103 tasks      | elapsed:   17.6s\n",
      "[Parallel(n_jobs=5)]: Done 118 tasks      | elapsed:   18.8s\n",
      "[Parallel(n_jobs=5)]: Done 135 tasks      | elapsed:   19.9s\n",
      "[Parallel(n_jobs=5)]: Done 152 tasks      | elapsed:   21.2s\n",
      "[Parallel(n_jobs=5)]: Done 171 tasks      | elapsed:   22.6s\n",
      "[Parallel(n_jobs=5)]: Done 190 tasks      | elapsed:   23.8s\n",
      "[Parallel(n_jobs=5)]: Done 211 tasks      | elapsed:   25.2s\n",
      "[Parallel(n_jobs=5)]: Done 240 out of 240 | elapsed:   27.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuned hyperparameters :(best parameters)  {'alpha': 0.0001, 'loss': 'log', 'max_iter': 1000, 'penalty': 'l2'}\n",
      "accuracy : 0.9785769291214835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
       "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=100,\n",
       "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
       "              random_state=None, shuffle=True, tol=0.001,\n",
       "              validation_fraction=0.1, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 357,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import parfit.parfit as pf\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "#Hyperparameter tuning of sgd with log loss(i.e logistic regression).\n",
    "\n",
    "grid = {\n",
    "    'alpha': [1e-4, 1e-3, 1e-2, 1e-1, 1e0, 1e1, 1e2, 1e3],\n",
    "    'max_iter': [100,500,1000], \n",
    "    'loss': ['log'], # logistic regression,\n",
    "    'penalty': ['l2'],\n",
    "    }\n",
    "\n",
    "sgd=SGDClassifier()\n",
    "sgd_cv=GridSearchCV(sgd,grid,cv=10,n_jobs=5,verbose=10)\n",
    "sgd_cv.fit(X_train,y_train)\n",
    "print(\"tuned hyperparameters :(best parameters) \",sgd_cv.best_params_)\n",
    "print(\"accuracy :\",sgd_cv.best_score_)\n",
    "\n",
    "sgd = SGDClassifier(alpha = 0.0001, loss ='log', max_iter = 100, penalty = 'l2')\n",
    "sgd.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = pd.read_csv('tableheader (3).csv',header = None,error_bad_lines=False,sep = ';')\n",
    "l.drop(0,axis = 1,inplace = True)\n",
    "l.fillna(0,inplace = True)\n",
    "l.columns = ['Id_doc','max_pages','pageN','maxTables','table_id','tabN','tRowsCount','HCellsCount','HCellTxt','DTagsStr','TtagsStr','clmncellTxt']\n",
    "l = l.loc[l['TtagsStr'].isin(['Horizontal*','NotPertinent*','Vertical*Interlaced*','Vertical*','Horizontal*Interlaced*','Interlaced*Vertical*','Interlaced*Horizontal*'])]\n",
    "l.drop(['DTagsStr','table_id'],axis = 1,inplace = True)\n",
    "l.loc[l['TtagsStr'].isin(['Vertical*Interlaced*','Interlaced*Vertical*','Vertical*']),'TtagsStr'] = 'Vertical'\n",
    "l.loc[l['TtagsStr'].isin(['Horizontal*','Horizontal*Interlaced*','Interlaced*Horizontal*']),'TtagsStr'] = 'Horizontal'\n",
    "l.loc[l['TtagsStr']=='NotPertinent*','TtagsStr'] = 'NotPertinent'\n",
    "z = l.copy()\n",
    "l['TtagsStr'] = np.where(l['TtagsStr'].isin(['NotPertinent']),'NotPertinent','Pertinent')\n",
    "l['TtagsStr'] = np.where(l['TtagsStr'].isin(['NotPertinent']),1,0)\n",
    "l = l.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []\n",
    "for row in range(len(l.index)):\n",
    "    headlines.append(' '.join(str(x) for x in l.iloc[row,[7,9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "corpus1 = []\n",
    "for i in range(len(headlines)):\n",
    "    rev = re.sub('[^a-zA-Z]',' ',headlines[i])\n",
    "    rev = rev.lower()\n",
    "    rev = rev.split()\n",
    "    rev = [ps.stem(word) for word in rev if word not in set(stopwords.words('italian'))]\n",
    "    rev = ' '.join(rev)\n",
    "    corpus1.append(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vectors = count_vectorizer.transform(corpus1).toarray()\n",
    "test_vectors = np.concatenate([test_vectors,np.array(l.iloc[:,[1,2,3,4,5,6]])],axis = 1) #Adding tables information into learningset\n",
    "y = l['TtagsStr']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9760435505156655, 0.9572685004154651)"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict_proba(test_vectors)\n",
    "y_pred1 = model.predict(test_vectors)\n",
    "roc_auc_score(y,y_pred[:,1]) , roc_auc_score(y,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = pd.concat([l,pd.Series(np.array(y_pred[:,1]),name = 'Predicted_proba'),pd.Series(np.array(y_pred1),name = 'Predicted_class')],axis = 1)\n",
    "f['TtagsStr'] = np.where(f['TtagsStr'].isin([0]),'NotPertinent','Pertinent')\n",
    "f['Predicted_class'] = np.where(f['Predicted_class'].isin([0]),'NotPertinent','Pertinent')\n",
    "f = f.iloc[:,[0,7,8,9,10,11]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id_doc</th>\n",
       "      <th>HCellTxt</th>\n",
       "      <th>TtagsStr</th>\n",
       "      <th>clmncellTxt</th>\n",
       "      <th>Predicted_proba</th>\n",
       "      <th>Predicted_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pagemento 2020/SETFFANO/20200105_1437_SETFFANO...</td>\n",
       "      <td>Polizza n. : *23743 *RINNOVO</td>\n",
       "      <td>NotPertinent</td>\n",
       "      <td>Polizza n. : *Rata del : *Importo. *Contraente...</td>\n",
       "      <td>5.660413e-02</td>\n",
       "      <td>NotPertinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pagemento 2021/ASSISTUDIO/20210129_1116_ASSIST...</td>\n",
       "      <td>polizza n. *47318494</td>\n",
       "      <td>NotPertinent</td>\n",
       "      <td>polizza n. *effetto *contraente *premio *</td>\n",
       "      <td>1.242251e-01</td>\n",
       "      <td>NotPertinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Pagemento 2021/IBK/20210115_1720_IBK del 1/15/21</td>\n",
       "      <td>Data *Descrizione ramo *Ramo *Titolo *Polizza ...</td>\n",
       "      <td>NotPertinent</td>\n",
       "      <td>Data *15/01/2021 *15/01/2021 *15/01/2021 *15/0...</td>\n",
       "      <td>3.739353e-12</td>\n",
       "      <td>NotPertinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Pagemento 2021/IBK/20210115_1720_IBK del 1/15/21</td>\n",
       "      <td>Data *Descrizione ramo *Ramo *Titolo *Polizza ...</td>\n",
       "      <td>NotPertinent</td>\n",
       "      <td>Data *15/01/2021 *15/01/2021 *15/01/2021 *15/0...</td>\n",
       "      <td>2.690813e-12</td>\n",
       "      <td>NotPertinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pagemento 2021/IBK/20210115_1720_IBK del 1/15/21</td>\n",
       "      <td>Data *Descrizione ramo *Ramo *Titolo *Polizza ...</td>\n",
       "      <td>NotPertinent</td>\n",
       "      <td>Data *15/01/2021 *15/01/2021 *15/01/2021 *15/0...</td>\n",
       "      <td>1.973126e-12</td>\n",
       "      <td>NotPertinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3146</th>\n",
       "      <td>Pagemento 2020/ABI/20200107_1850_ABI_00099BE1 ...</td>\n",
       "      <td>Area Brokers Industria S.r.t. *</td>\n",
       "      <td>Pertinent</td>\n",
       "      <td>Area Brokers Industria S.r.t. *Corso Venezia, ...</td>\n",
       "      <td>9.999457e-01</td>\n",
       "      <td>Pertinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3147</th>\n",
       "      <td>Pagemento 2020/AON/20200107_2254_AON_00099BE6 ...</td>\n",
       "      <td>Fax: *025351521</td>\n",
       "      <td>Pertinent</td>\n",
       "      <td>Fax: *From: *Date: *Pages: *Subject: *</td>\n",
       "      <td>9.964570e-01</td>\n",
       "      <td>Pertinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3148</th>\n",
       "      <td>Pagemento 2020/AON/20200107_2254_AON_00099BE6 ...</td>\n",
       "      <td>Filiale *Cliente *Antiriciclaggio *Ramo *Nro C...</td>\n",
       "      <td>NotPertinent</td>\n",
       "      <td>Filiale *Cargo ***PARMA Viale Mentana, 45 *</td>\n",
       "      <td>7.197022e-04</td>\n",
       "      <td>NotPertinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3149</th>\n",
       "      <td>Pagemento 2020/AON/20200107_2254_AON_00099BE6 ...</td>\n",
       "      <td>Filiale *Cliente *Antiriciclaggio *Ramo *Nro C...</td>\n",
       "      <td>NotPertinent</td>\n",
       "      <td>Filiale *MILANO Via Calindri, 6 **PARMA Viale ...</td>\n",
       "      <td>5.356172e-03</td>\n",
       "      <td>NotPertinent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3150</th>\n",
       "      <td>Pagemento 2021/EB/20210107_1756_EB del 1/7/21</td>\n",
       "      <td>Cliente *T *Polizza *Ramo *Premio *Effetto *Ta...</td>\n",
       "      <td>NotPertinent</td>\n",
       "      <td>Cliente *COVENTYA SPA *MACCARESE SPA *MACCARES...</td>\n",
       "      <td>4.073239e-02</td>\n",
       "      <td>NotPertinent</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3151 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 Id_doc  \\\n",
       "0     Pagemento 2020/SETFFANO/20200105_1437_SETFFANO...   \n",
       "1     Pagemento 2021/ASSISTUDIO/20210129_1116_ASSIST...   \n",
       "2     Pagemento 2021/IBK/20210115_1720_IBK del 1/15/21    \n",
       "3     Pagemento 2021/IBK/20210115_1720_IBK del 1/15/21    \n",
       "4     Pagemento 2021/IBK/20210115_1720_IBK del 1/15/21    \n",
       "...                                                 ...   \n",
       "3146  Pagemento 2020/ABI/20200107_1850_ABI_00099BE1 ...   \n",
       "3147  Pagemento 2020/AON/20200107_2254_AON_00099BE6 ...   \n",
       "3148  Pagemento 2020/AON/20200107_2254_AON_00099BE6 ...   \n",
       "3149  Pagemento 2020/AON/20200107_2254_AON_00099BE6 ...   \n",
       "3150     Pagemento 2021/EB/20210107_1756_EB del 1/7/21    \n",
       "\n",
       "                                               HCellTxt      TtagsStr  \\\n",
       "0                         Polizza n. : *23743 *RINNOVO   NotPertinent   \n",
       "1                                 polizza n. *47318494   NotPertinent   \n",
       "2     Data *Descrizione ramo *Ramo *Titolo *Polizza ...  NotPertinent   \n",
       "3     Data *Descrizione ramo *Ramo *Titolo *Polizza ...  NotPertinent   \n",
       "4     Data *Descrizione ramo *Ramo *Titolo *Polizza ...  NotPertinent   \n",
       "...                                                 ...           ...   \n",
       "3146                    Area Brokers Industria S.r.t. *     Pertinent   \n",
       "3147                                   Fax: *025351521      Pertinent   \n",
       "3148  Filiale *Cliente *Antiriciclaggio *Ramo *Nro C...  NotPertinent   \n",
       "3149  Filiale *Cliente *Antiriciclaggio *Ramo *Nro C...  NotPertinent   \n",
       "3150  Cliente *T *Polizza *Ramo *Premio *Effetto *Ta...  NotPertinent   \n",
       "\n",
       "                                            clmncellTxt  Predicted_proba  \\\n",
       "0     Polizza n. : *Rata del : *Importo. *Contraente...     5.660413e-02   \n",
       "1             polizza n. *effetto *contraente *premio *     1.242251e-01   \n",
       "2     Data *15/01/2021 *15/01/2021 *15/01/2021 *15/0...     3.739353e-12   \n",
       "3     Data *15/01/2021 *15/01/2021 *15/01/2021 *15/0...     2.690813e-12   \n",
       "4     Data *15/01/2021 *15/01/2021 *15/01/2021 *15/0...     1.973126e-12   \n",
       "...                                                 ...              ...   \n",
       "3146  Area Brokers Industria S.r.t. *Corso Venezia, ...     9.999457e-01   \n",
       "3147             Fax: *From: *Date: *Pages: *Subject: *     9.964570e-01   \n",
       "3148        Filiale *Cargo ***PARMA Viale Mentana, 45 *     7.197022e-04   \n",
       "3149  Filiale *MILANO Via Calindri, 6 **PARMA Viale ...     5.356172e-03   \n",
       "3150  Cliente *COVENTYA SPA *MACCARESE SPA *MACCARES...     4.073239e-02   \n",
       "\n",
       "     Predicted_class  \n",
       "0       NotPertinent  \n",
       "1       NotPertinent  \n",
       "2       NotPertinent  \n",
       "3       NotPertinent  \n",
       "4       NotPertinent  \n",
       "...              ...  \n",
       "3146       Pertinent  \n",
       "3147       Pertinent  \n",
       "3148    NotPertinent  \n",
       "3149    NotPertinent  \n",
       "3150    NotPertinent  \n",
       "\n",
       "[3151 rows x 6 columns]"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = pd.DataFrame(data = f , columns = ['Id_doc','TtagsStr','Predicted_class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_predicted = summary.loc[summary['Predicted_class'] !=  summary['TtagsStr']]['Id_doc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30      Pagemento 2021/WIDE/20210107_1712_WIDE del 1/7...\n",
       "31      Pagemento 2021/WIDE/20210107_1712_WIDE del 1/7...\n",
       "39      Pagemento 2021/BLUBROKER/20210121_1805_BLUBROK...\n",
       "41      Pagemento 2021/INPROJECT VITA/20210108_1550_IN...\n",
       "47      Pagemento 2020/_WIDE/20201231__WIDE del 12/31/20 \n",
       "                              ...                        \n",
       "3103       Pagemento 2021/PICO/20210119 PICO del 1/19/21 \n",
       "3118     Pagemento 2021/ABI/20210125_1858_ABI del 1/25/21\n",
       "3129    Pagemento 1/ABI/29219121_1922_ABI_00099DF5 del...\n",
       "3133    Pagemento 1/in_attesa/ABI in attesa di rispost...\n",
       "3139    Pagemento 2020/ABI/20200105_1903_ABI_00099BAB ...\n",
       "Name: Id_doc, Length: 188, dtype: object"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "incorrect_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "incorrect_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('tableheader (3).csv',header = None,error_bad_lines=False,sep = ';')\n",
    "df1.drop(df1.iloc[:,0:2] ,axis = 1,inplace = True)\n",
    "df1.fillna(0,inplace = True)\n",
    "df1.columns = ['max_pages','pageN','maxTables','table_id','tabN','tRowsCount','HCellsCount','HCellTxt','DTagsStr','TtagsStr','clmncellTxt']\n",
    "df1 = df1.loc[df1['TtagsStr'].isin(['Horizontal*','NotPertinent*','Vertical*Interlaced*','Vertical*','Horizontal*Interlaced*','Interlaced*Vertical*','Interlaced*Horizontal*'])]\n",
    "df1.drop(['DTagsStr','table_id'],axis = 1,inplace = True)\n",
    "df1.loc[df1['TtagsStr'].isin(['Vertical*Interlaced*','Interlaced*Vertical*','Vertical*']),'TtagsStr'] = 'Vertical'\n",
    "df1.loc[df1['TtagsStr'].isin(['Horizontal*','Horizontal*Interlaced*','Interlaced*Horizontal*']),'TtagsStr'] = 'Horizontal'\n",
    "df1.loc[df1['TtagsStr']=='NotPertinent*','TtagsStr'] = 'NotPertinent'\n",
    "df = df1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.loc[df1['TtagsStr'] == 'Vertical' , 'TtagsStr'] = 0\n",
    "df1.loc[df1['TtagsStr'] == 'Horizontal' , 'TtagsStr'] = 1\n",
    "df1.loc[df1['TtagsStr'] == 'NotPertinent' , 'TtagsStr'] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines = []\n",
    "for row in range(len(df1.index)):\n",
    "    headlines.append(' '.join(str(x) for x in df1.iloc[row,[6,8]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "corpus1 = []\n",
    "for i in range(len(headlines)):\n",
    "    rev = re.sub('[^a-zA-Z]',' ',headlines[i])\n",
    "    rev = rev.lower()\n",
    "    rev = rev.split()\n",
    "    rev = [ps.stem(word) for word in rev if word not in set(stopwords.words('italian'))]\n",
    "    rev = ' '.join(rev)\n",
    "    corpus1.append(rev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "train_vectors = count_vectorizer.fit_transform(corpus1)\n",
    "X = train_vectors.toarray()\n",
    "y = df1['TtagsStr'].apply(lambda x:int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.concatenate([X,np.array(df1.iloc[:,[0,1,2,3,4,5]])],axis = 1) #Adding tables information into learningset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state = 42)\n",
    "model = SGDClassifier().fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f1_score(y_test,y_pred, average='macro'))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_length=20\n",
    "voc_size = 30000\n",
    "onehot_repr=[one_hot(words,voc_size)for words in corpus1]\n",
    "embedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n",
    "embedding_vector_features=30\n",
    "model=Sequential()\n",
    "model.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(1,activation='relu'))\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics='accuracy')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final=np.array(embedded_docs)\n",
    "y_final=np.array(y)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.2, random_state=42)\n",
    "model.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=2,batch_size=32)\n",
    "y_pred3 = model.predict_classes(X_test)\n",
    "y_pred3\n",
    "print(f1_score(y_test,y_pred3, average='micro'))\n",
    "print(confusion_matrix(y_test,y_pred3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
